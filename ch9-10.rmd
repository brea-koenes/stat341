---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(ggformula)
library(dplyr)
library(bayesplot)
library(tidybayes)
library(posterior)
library(janitor)
library(rstan)
library(broom.mixed)
```

# Chapter 9

## 9.9

a.  

```{r}
summarize_normal_normal(mean = 1000, sd = 2000, sigma = 10,
                        y_bar = 9000, n = 10000)
```

b.  

```{r}
require(rstanarm)

model <- stan_glm(
  rides ~ humidity, data = bikes, 
  family = gaussian,
  prior_PD = TRUE,
  prior_intercept = normal(1000, 2000, autoscale = TRUE),
  prior = normal(0, 10, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 5, iter = 8000, seed = 84735)
```

c.  

```{r}
require(bayesplot)

# Trace plots of parallel chains
mcmc_trace(model, size = .1)

# Density plots of parallel chains
mcmc_dens_overlay(model)
```

d.  

Ridership decreases as humidity increases. There is much variability with the ridership per day, so that impacts the strength of the relationship.

## 9.10

a.  

```{r}
# Load and plot data
data(bikes)
ggplot(bikes, aes(x = humidity, y = rides)) + 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```

b.  

The data is normally distributed enough; the points are fairly evenly distributed across the y-axis. Since it doesn't form a trumpet-like shape, it could be fine for Normal regression.

## 9.11

a.  

```{r}
bike_model <- stan_glm(rides ~ humidity, data = bikes, 
  family = gaussian,
  prior_PD = FALSE,
  prior_intercept = normal(1000, 2000, autoscale = TRUE),
  prior = normal(0, 10, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 5, iter = 4000*2, seed = 84735)
```

b.  

```{r}
# Effective sample size ratio 
neff_ratio(bike_model)
```

```{r}
# Density plots of parallel chains
mcmc_dens_overlay(bike_model)
```

Given the results above, we can trust these simulation results.

c.  

```{r}
# Trace plots of parallel chains
mcmc_trace(bike_model, size = .1)
```

The sigma is more centered than 9.9's and the scales are much different.

## 9.12

a.  

```{r}
tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.95)
```

b.  

The posterior median relationship is 3935.315 - 8.128. That means for every one degree increase in humidity, we expect ridership to decrease by about 8 rides.

c.  

The CI represents the uncertainty in the relationship. $$\beta_1$$ (-16.16035,1.899641) indicates that this slope could range anywhere between about -16 and 2.

d.  

I would say that we have alright posterior evidence that there's a negative association between ridership and humidity. The standard error is low enough for it to carry some weight, but not low enough for there to be very certain evidence to their relationship. However, the CI is a small range, so that does help the credibility of the evidence of their relationship.

## 9.16

a.  

```{r}
summary(penguins_bayes)
```


```{r}
penguins_model <- stan_glm(flipper_length_mm ~ bill_length_mm, data = penguins_bayes,
                       family = gaussian,
                       prior_PD = TRUE, 
                       prior_intercept = normal(43.92, 11.82),
                       prior = normal(200.9, 28.9), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 10000*2, seed = 84735)
```

b.  

```{r}
prior_summary(penguins_model)
```

c.  

```{r}
# Trace plots of parallel chains
mcmc_trace(penguins_model, size = 0.1)
```

d.  

It looks like flipper and bill length may be negatively associated.

## 9.17

a.  

```{r}
penguins_bayes |>
  ggplot(aes(x = flipper_length_mm, y = bill_length_mm)) +
  geom_point()
```

There appears to be a positive linear relationship between flipper and bill length.

b.  

It meets the conditions for a simple normal regression model. It has linearity on the plot above. If it also has independent observations and is normally distributed on a fitted vs resids plot, we can use a normal model.

## 9.18

a.  

```{r}
penguins_post_model <- stan_glm(flipper_length_mm ~ bill_length_mm, data = penguins_bayes,
                       family = gaussian,
                       prior_intercept = normal(43.92, 11.82),
                       prior = normal(200.9, 28.9), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 10000*2, seed = 84735)
```

b.  

```{r}
# Trace plots of parallel chains
mcmc_trace(penguins_post_model, size = 0.1)
```

c.  

```{r}
tidy(penguins_post_model, conf.int = TRUE, conf.level = 0.90)
```

d.  

There is a 90% probability that the true bill_length_mm lies within .1.5-.1.9. 

e.  

There is a decent standard error, so I'm wary to say that we have enough evidence that they are related.

# Chapter 10

## 10.7

a.  

```{r problem7}
problem7_data <- 
  tibble(
    x = c(12,10,4,8,6), 
    y = c(20,17,4,11,9)
  )
```

$$

y=mx+b \\

y= \beta\_1x+\beta\_0 \\

y= \beta\_0 + \beta\_1x \\

\hat{y}\_i= \beta\_0 + \beta\_1 x_i \\

\hat{y}\_1= -1.8 + 2.1 \cdot 12 \\

\hat{y}\_2= -1.8 + 2.1 \cdot 10 \\

$$

b.  

```{r}
problem7_data |>
  mutate(y_hat=-1.8+2.1*x, 
         y_model=-1.8+2.1*x+rnorm(5,0,0.8),
         y_model2=rnorm(5,-1.8+2.1*x,0.8)
        )
```

## 10.13

a.  

```{r}
head(coffee_ratings)
```

It likely violates the independence assumption due to aroma and aftertaste not being completely independent. Also, other variables come into play, such as sweetness, clean_cup, and acidity.

b.  

```{r}
set.seed(84735)
new_coffee <- coffee_ratings |> 
  group_by(farm_name) |> 
  sample_n(1) |> 
  ungroup()
dim(new_coffee)
```

## 10.14

a.  

```{r}
ggplot(new_coffee, aes(x = aroma, y = total_cup_points)) + 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```

b.  

```{r}
summary(new_coffee)
```

```{r}
coffee_model <- stan_glm(total_cup_points ~ aroma, data = new_coffee,
                       family = gaussian,
                       prior_intercept = normal(7.5, 1.2),
                       prior = normal(82.10, 30), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 10000*2, seed = 84735)
```

c.  

```{r}
mcmc_trace(coffee_model, size = 0.1)
```

d.  

```{r}
summary(coffee_model)
```

e.  

Aroma and rating are associated. The better a coffee's aroma is, the rating is likely to be better as well.

## 10.15

a.  

```{r}
first_set <- head(coffee_model, 1)
first_set

set.seed(84735)
one_simulation <- new_coffee |> 
  mutate(mu = -65 + 11 * 11,
         simulated_coffee = rnorm(572, mean = mu, sd = 62))
```

b.  

```{r}
ggplot(one_simulation, aes(x = simulated_coffee)) + 
  geom_density(color = "red") + 
  geom_density(aes(x = aroma), color = "darkblue")
```

c.  

```{r}
pp_check(coffee_model, nreps = 50) + 
  xlab("aroma")
```

d.  

Assumption 2 is that the relationship is linear. 

```{r}
ggplot(new_coffee, aes(x = aroma, y = total_cup_points)) + 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```

The relationship is basically linear. This passes this assumption.

Assumption 3 is that the variability in Y does not increase with X. As seen in the previous density plot, Y does not increase with X. It passes this assumption as well.

## 10.16

a.  

```{r}
set.seed(84735)
one_simulation2 <- new_coffee |> 
  mutate(mu = -65 + 11 * 7.67,
         simulated_coffee = rnorm(572, mean = mu, sd = 62))
```

b.  

```{r}
ggplot(one_simulation2, aes(x = simulated_coffee)) + 
  geom_density(color = "red") + 
  geom_density(aes(x = aroma), color = "darkblue")
```

c.  

```{r}
set.seed(84735)
predictions <- posterior_predict(coffee_model, newdata = new_coffee)

ppc_intervals(new_coffee$total_cup_points, yrep = predictions, x = new_coffee$aroma, 
              prob = 0.5, prob_outer = 0.95)
```

d.  

All batches have ratings that are within their 50% posterior prediction interval (light blue line).

## 10.17

a.  

```{r}
set.seed(84735)
cv_procedure <- prediction_summary_cv(
  model = coffee_model, data = new_coffee, k = 10)
cv_procedure
```

b.  

MAE is the mean average error per fold, which measures the prediction quality. This measure displays the typical difference between the observed Y_i and their posterior predictive means.

MAE scaled is the mean average error that measures the typical number of standard deviations that the observed Y_i fall from their posterior predictive means. Its calculation is different from MAE due to dividing by standard deviation.

Within 50 measures the proportion of observed Y_i that are within their 50% posterior prediction interval. 

Within 95 measures the proportion of observed Y_i that are within their 95% posterior prediction interval.

c.  

```{r}
set.seed(84735)
prediction_summary(coffee_model, data = new_coffee)
```

The reported cross-validated MAE is verified given the information above from the 10 folds. 

## 10.18

The stated typical prediction error is 61. Our model's is 65. Given this, I would say that our model is pretty fair.

## 10.19

a.  

```{r}
coffee_model3 <- stan_glm(total_cup_points ~ aftertaste, data = new_coffee,
                       family = gaussian,
                       prior_intercept = normal(7.5, 1.2),
                       prior = normal(82.10, 30), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 10000*2, seed = 84735)
```

b.  

```{r}
ggplot(new_coffee, aes(y = total_cup_points, x = aftertaste)) + 
  geom_point(size = 0.2) + 
  geom_smooth(method = "lm", se = FALSE)
```

It passes the linearity assumption, #2.

```{r}
pp_check(coffee_model3, nreps = 50) + 
  xlab("aroma")
```

It passes assumption #3.

c.  

```{r}
set.seed(84735)
cv_procedure2 <- prediction_summary_cv(
  model = coffee_model3, data = new_coffee, k = 10)
cv_procedure2
```

d.  

Aftertaste has a stronger association with total_cup_points, so I would choose aftertaste.
