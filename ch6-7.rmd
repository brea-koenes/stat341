---
title: "ch6-7.rmd"
author: "Brea Koenes"
date: "10/24/2022"
output: html_document
---

```{r setup, include=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(ggformula)
library(dplyr)
```

# Chapter 6

## 6.5

a.  

```{r}
# Define grid
grid_data <- data.frame(pi = seq(from = 0, to = 1, length = 5))

# Evaluate the prior & likelihood at each pi
grid_data <- grid_data |> 
  mutate(prior = dbeta(pi, 3, 8),
         likelihood = dbinom(2, 10, pi))

# Approximate the posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 1000, 
                        weight = posterior, replace = TRUE)

post_sample |>
  gf_dhistogram(~pi)
```

b.  

```{r}
# Define grid
grid_data <- data.frame(pi = seq(from = 0, to = 1, length = 201))

# Evaluate the prior & likelihood at each pi
grid_data <- grid_data |> 
  mutate(prior = dbeta(pi, 3, 8),
         likelihood = dbinom(2, 10, pi))

# Approximate the posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

gf_dhistogram(~pi, data=post_sample) |>
  gf_dist("beta", 3+2, 8+8)
```

## 6.6

a.  

```{r}
# Define grid
grid_data   <- data.frame(lambda = seq(from = 0, to = 8, length = 90))

# Evaluate prior & likelihood
grid_data <- grid_data |> 
  mutate(prior = dgamma(lambda, 20, 5),
         likelihood = prod(dpois(c(0,1,0), lambda)))

# Approximate posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

# Histogram of the grid simulation with posterior pdf 
ggplot(post_sample, aes(x = lambda)) + 
  geom_histogram(aes(y = stat(density))) + 
  stat_function(fun = dgamma, args = list(20, 5)) + 
  lims(x = c(0, 8))
```

b.  

```{r}
# Define grid 
grid_data   <- data.frame(lambda = seq(from = 0, to = 8, length = 201))

# Evaluate prior & likelihood
grid_data <- grid_data |> 
  mutate(prior = dgamma(lambda, 20, 5),
         likelihood = prod(dpois(c(0,1,0), lambda)))

# Approximate the posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

# Histogram of the grid simulation with posterior pdf 
ggplot(post_sample, aes(x = lambda)) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun = dgamma, args = list(20, 5)) + 
  lims(x = c(0, 8))
```

## modified 6.7

```{r}
y = c(7.1,8.9,8.4,8.6)

grid <- expand.grid(
  mu = seq(4, 16, by=.05), 
  sigma = seq(.1, 4, by=.05)) |>
  mutate(
      prior = dnorm(mu,10,2)*dgamma(sigma, 2, 2),
      liklihood = purrr::map2_dbl(mu, sigma, ~prod(dnorm(y, .x, .y))),
      posterior = prior*liklihood
  )
```

```{r}
set.seed(84735)

post_sample <- sample_n(grid, size = 10000, 
                        weight = posterior, replace = TRUE)

gf_density2d(mu~sigma, data=post_sample)
```

## 6.13

a.  

```{r}
# Define model
bb_model <- "
  data {
    int<lower = 0, upper = 10> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(10, pi);
    pi ~ beta(3, 8);
  }
"
```

```{r echo=FALSE}
# Simulate the posterior
require(rstan)
bb_sim <- stan(model_code = bb_model, data = list(Y = 2), 
               chains = 3, iter = (12000*2)/3, seed = 84735)
```

b.  

```{r}
require(bayesplot)
mcmc_trace(bb_sim, pars = "pi", size = 0.1)
```

c.  

The range is 0 to 4000. The maximum in the range of values is not 12000 because some iterations are used for the warmup.

d.  

```{r}
# Density plot of the Markov chain values
mcmc_dens(bb_sim, pars = "pi") + 
  yaxis_text(TRUE) + 
  ylab("density")
```

e.  

```{r}
bayesrules::summarize_beta(alpha = 3, beta = 8)
```

```{r}
summarize_normal_normal(mean = 8, sd = .13, sigma = pi, y_bar = 8.27, n = 10)
```

My MCMC approximation is very similar to the results from the posterior model of pi. They have a similar mode.

## 6.16

a.  

```{r}
# Define model
gp_model <- "
  data {
    int<lower = 0> Y[3];
  }
  parameters {
    real<lower = 0> lambda;
  }
  model {
    Y ~ poisson(lambda); 
    lambda ~ gamma(5, 5);
  }
"

# Simulate the posterior
gp_sim <- stan(model_code = gp_model, data = list(Y = c(0,1,0)), 
               chains = 4, iter = (10000*2)/4, seed = 84735)
```

b.  

```{r}
# Trace plots of the 4 Markov chains
mcmc_trace(gp_sim, pars = "lambda", size = 0.1)

# Density plot of the Markov chain values
mcmc_dens(gp_sim, pars = "lambda") + 
  yaxis_text(TRUE) + 
  ylab("density")
```

c.  

The most plausible posterior appears to be about 0.65.

d.  

```{r}
summarize_gamma_poisson(shape = 5, rate = 5, sum_y = 1, n = 3)
```

My MCMC approximation is closer to the prior than approximating using a posterior model.

## modified 6.17

```{r}
require(rstan)
model <- "
  data {
    real y[4];
  }
  parameters {
    real mu;
    real<lower=0> sigma;
  }
  model {
    y ~ normal(mu, sigma);
    mu ~ normal(10, 2);
    sigma ~ gamma(2,2);
  }
"

data = list(y=c(7.1,8.9,8.4,8.6))

sim <- stan(model_code = model, data = data, iter = 5000*2, seed = 84735)
plot(sim)
```

# Chapter 7

## 7.6

a.  

```{r}
current <- 4.6

set.seed(84735)

proposal <- rnorm(1, current, 2)
proposal
```

b.  

```{r}
current <- 2.1

set.seed(84735)

proposal <- rnorm(1, current, 7)
proposal
```

c.  

```{r}
current <- 8.9

set.seed(84735)

proposal <- runif(1, min = current - 2, max = current + 2)
proposal
```

d.  

```{r}
current <- 1.2

set.seed(84735)

proposal <- runif(1, min = current - 0.5, max = current + 0.5)
proposal
```

e.  

```{r}
current <- 7.7

set.seed(84735)

proposal <- runif(1, min = current - 3, max = current + 3)
proposal
```

## 7.7

a.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {lambda^(-2)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

b.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {exp(lambda)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

c.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {exp(-10*lambda)}

proposal_plaus <- posterior(proposal) * dunif(1, min = current - 0.5, max = current + 0.5)

current_plaus  <- posterior(current) * dunif(1, min = current - 0.5, max = current + 0.5)

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

d.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {exp((-lambda)^4)} 

proposal_plaus <- posterior(proposal) * dexp(current, rate = proposal)

current_plaus  <- posterior(current) * dexp(proposal, rate = current)

alpha <- min(1, proposal_plaus / current_plaus) 
alpha
```

e.  

The scenario with the uniform distribution has an acceptance probability of 1, which we automatically accept.

When a proposal is close to the current location, they will have a similar posterior plausibility and will most likely be accepted; that is why so many of the acceptance probabilities above are high.

## 7.8

a.  

```{r}
current <- 1.8
proposal <- 1.6

proposal_plaus <- dnorm(proposal, 0, 2)

current_plaus  <- dnorm(current, 0, 2) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

b.  

```{r}
current <- 1.8
proposal <- 1.6

proposal_plaus <- dnorm(proposal, 0, .5)

current_plaus  <- dnorm(current, 0, .5) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

c.  

```{r}
current <- 1.8
proposal <- 1.6

proposal_plaus <- dnorm(proposal, 0, 1) * bayestestR::distribution_uniform(-0.3,0.3)

current_plaus  <- dnorm(current, 0, 1) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

d.  

```{r}
current <- 1.8
proposal <- 1.6

proposal_plaus <- dnorm(proposal, 0, 4)

current_plaus  <- dnorm(current, 0, 4) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

e.  

They are all 100% acceptance probability, maybe becuase the chain has already been at that location so it must have already been accepted.

## 7.10

a.  

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour <- function(N, w){
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(0, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_iteration(w = w, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), mu))
}
```

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 50, w = 50)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

b.  

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour <- function(N, w){
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(0, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_iteration(w = w, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), mu))
}
```

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 50, w = 0.1)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

c.  

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour <- function(N, w){
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(0, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_iteration(w = w, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), mu))
}
```

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 1000, w = 50)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

d.  

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour <- function(N, w){
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(0, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_iteration(w = w, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), mu))
}
```

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 1000, w = 0.1)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

e.  

"w" is the the Uniform half-width. Due to this, it makes sense that a w value will result in a more detailed plot. A larger w will result in a chunkier plot with less detail across iterations. This can be seen in comparing a and b. a, with a larger w, is has a chunky, stark line. b has much more detail and variation across the line, as it has a smaller w.

f.  

Having a low w is important in scenarios with more iterations. As seen in c and d, plot d has much more detail and gives us more information about the variation between iterations than c does---it has a low w.

## 7.12

a.  

```{r}
one_mh_normal_iteration <- function(s, current){
 # STEP 1: Propose the next chain location
 proposal <- rnorm(1, mean = current, sd = s)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}

mh_tour_normal <- function(N, s){
  
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(NA, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_normal_iteration(s = s, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = 1:N, mu))
}
```

```{r}
set.seed(84735)
mh_tour_normal(20, 0.01) |>
  gf_line(mu~iteration)
```

b.  

```{r}
set.seed(84735)
mh_tour_normal(20, 10) |>
  gf_line(mu~iteration)
```

c.  

```{r}
set.seed(84735)
mh_tour_normal(1000, 0.01) |>
  gf_line(mu~iteration)
```

d.  

```{r}
set.seed(84735)
mh_tour_normal(1000, 10) |>
  gf_line(mu~iteration)
```

e.  

The standard deviation dictates how many deviations from the current value that the next chain's value will be predicted from. When changing the standard deviation to a larger number, the range that the next predicted value will be in is broadened. This is why we see a larger range of values and places where the line is flat in b's plot compared to a.

f.  

```{r}
set.seed(84735)
mh_tour_normal(1000, 1) |>
  gf_line(mu~iteration)
```

1 would be a reasonable standard deviation, as seen in the plot above.

## 7.14

a.  

Exponential because it is greater than 0.

b.  

```{r}
one_iteration <- function(a, b, current){
 # Propose the next chain location
 proposal <- rbeta(1, a, b)
  
 # Decide whether or not to go there
 proposal_plaus <- dbeta(proposal, 2, 3) * dbinom(1, 2, proposal)
 proposal_q     <- dbeta(proposal, a, b)
 current_plaus  <- dbeta(current, 2, 3) * dbinom(1, 2, current)
 current_q      <- dbeta(current, a, b)
 alpha <- min(1, proposal_plaus / current_plaus * current_q / proposal_q)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 return(data.frame(proposal, alpha, next_stop))}
```

```{r}
betabin_tour <- function(N, a, b){
  # 1. Start the chain at location 1
  current <- 1

  # 2. Initialize the simulation
  pi <- rep(0, N)
  
  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_iteration(a = a, b = b, current = current)
    
    # Record next location
    pi[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), pi))
}
```

```{r}
set.seed(84735)
betabin_sim <- betabin_tour(N = 1000, a = 1, b = .1)

ggplot(betabin_sim, aes(x = iteration, y = pi)) + 
  geom_line()
```

c.  

0.4

d.  

```{r}
# Plot the results
ggplot(betabin_sim, aes(x = iteration, y = pi)) + 
  geom_line()
ggplot(betabin_sim, aes(x = pi)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dbeta, args = list(3, 4), color = "blue")
```

It is not excellent at approximation.

## 7.15

prior: proportion of emails I receive 24 hours: 19/24

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour <- function(N, w){
  # start the chain at 19 emails
  current <- 19

  # initialize the simulation
  mu <- rep(0, N)

  # simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_iteration(w = w, current = current)
    
    # record next email
    mu[i] <- sim$next_stop
    
    # reset the current email
    current <- sim$next_stop
  }
  
  # return the chain emails
  return(data.frame(iteration = c(1:N), mu))
}

#  simulate a Markov chain tour of length 2000
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 2000, w = pi)

# plot
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()

ggplot(mh_simulation_1, aes(x = mu)) + 
  geom_histogram(aes(y = ..density..), color = "white", bins = 20) + 
  stat_function(fun = dnorm, args = list(4,0.6), color = "blue")
```

The trace plot (left) illustrates the email or sequence of emails stops. The histogram (right) illustrates the relative amount of emails per day.
