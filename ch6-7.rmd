---
title: "ch6-7.rmd"
author: "Brea Koenes"
date: "10/24/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(ggformula)
library(dplyr)
```

# Chapter 6

## 6.5

a.  

```{r}
# Define grid
grid_data <- data.frame(pi = seq(from = 0, to = 1, length = 5))

# Evaluate the prior & likelihood at each pi
grid_data <- grid_data |> 
  mutate(prior = dbeta(pi, 3, 8),
         likelihood = dbinom(2, 10, pi))

# Approximate the posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 1000, 
                        weight = posterior, replace = TRUE)

post_sample |>
  gf_dhistogram(~pi)
```

b.  

```{r}
# Define grid
grid_data <- data.frame(pi = seq(from = 0, to = 1, length = 201))

# Evaluate the prior & likelihood at each pi
grid_data <- grid_data |> 
  mutate(prior = dbeta(pi, 3, 8),
         likelihood = dbinom(2, 10, pi))

# Approximate the posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

gf_dhistogram(~pi, data=post_sample) |>
  gf_dist("beta", 3+2, 8+8)
```

## 6.6

a.  

```{r}
# Define grid
grid_data   <- data.frame(lambda = seq(from = 0, to = 8, length = 90))

# Evaluate prior & likelihood
grid_data <- grid_data |> 
  mutate(prior = dgamma(lambda, 20, 5),
         likelihood = prod(dpois(c(0,1,0), lambda)))

# Approximate posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

# Histogram of the grid simulation with posterior pdf 
ggplot(post_sample, aes(x = lambda)) + 
  geom_histogram(aes(y = stat(density))) + 
  stat_function(fun = dgamma, args = list(20, 5)) + 
  lims(x = c(0, 8))
```

b.  

```{r}
# Define grid 
grid_data   <- data.frame(lambda = seq(from = 0, to = 8, length = 201))

# Evaluate prior & likelihood
grid_data <- grid_data |> 
  mutate(prior = dgamma(lambda, 20, 5),
         likelihood = prod(dpois(c(0,1,0), lambda)))

# Approximate the posterior
grid_data <- grid_data |> 
  mutate(posterior = likelihood * prior)

set.seed(84735)

# Sample from the discretized posterior
post_sample <- sample_n(grid_data, size = 10000, 
                        weight = posterior, replace = TRUE)

# Histogram of the grid simulation with posterior pdf 
ggplot(post_sample, aes(x = lambda)) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun = dgamma, args = list(20, 5)) + 
  lims(x = c(0, 8))
```

## modified 6.7

```{r}
y = c(7.1,8.9,8.4,8.6)

grid <- expand.grid(
  mu = seq(4, 16, by=.05), 
  sigma = seq(.1, 4, by=.05)) |>
  mutate(
      prior = dnorm(mu,10,2)*dgamma(sigma, 2, 2),
      liklihood = purrr::map2_dbl(mu, sigma, ~prod(dnorm(y, .x, .y))),
      posterior = prior*liklihood
  )
```

```{r}
set.seed(84735)

post_sample <- sample_n(grid, size = 10000, 
                        weight = posterior, replace = TRUE)

gf_density2d(mu~sigma, data=post_sample)
```

## 6.13

a.  

```{r}
# Define model
bb_model <- "
  data {
    int<lower = 0, upper = 10> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(10, pi);
    pi ~ beta(3, 8);
  }
"
```

```{r echo=FALSE}
# Simulate the posterior
require(rstan)
bb_sim <- stan(model_code = bb_model, data = list(Y = 2), 
               chains = 3, iter = (12000*2)/3, seed = 84735)
```

b.  

```{r}
require(bayesplot)
mcmc_trace(bb_sim, pars = "pi", size = 0.1)
```

c.  

The range is 0 to 4000. The maximum in the range of values is not 12000 because some iterations are used for the warmup.

d.  

```{r}
# Density plot of the Markov chain values
mcmc_dens(bb_sim, pars = "pi") + 
  yaxis_text(TRUE) + 
  ylab("density")
```

e.  

```{r}
bayesrules::summarize_beta(alpha = 3, beta = 8)
```

```{r}
summarize_normal_normal(mean = 8, sd = .13, sigma = pi, y_bar = 8.27, n = 10)
```

My MCMC approximation is very similar to the results from the posterior model of pi. They have a similar mode.

## 6.16

a.  

```{r}
# Define model
gp_model <- "
  data {
    int<lower = 0> Y[3];
  }
  parameters {
    real<lower = 0> lambda;
  }
  model {
    Y ~ poisson(lambda); 
    lambda ~ gamma(5, 5);
  }
"

# Simulate the posterior
gp_sim <- stan(model_code = gp_model, data = list(Y = c(0,1,0)), 
               chains = 4, iter = (10000*2)/4, seed = 84735)
```

b.  

```{r}
# Trace plots of the 4 Markov chains
mcmc_trace(gp_sim, pars = "lambda", size = 0.1)

# Density plot of the Markov chain values
mcmc_dens(gp_sim, pars = "lambda") + 
  yaxis_text(TRUE) + 
  ylab("density")
```

c.  

The most plausible posterior appears to be about 0.65.

d.  

```{r}
summarize_gamma_poisson(shape = 5, rate = 5, sum_y = 1, n = 3)
```

My MCMC approximation is closer to the prior than approximating using a posterior model.

## modified 6.17

```{r}
require(rstan)
model <- "
  data {
    real y[4];
  }
  parameters {
    real mu;
    real<lower=0> sigma;
  }
  model {
    y ~ normal(mu, sigma);
    mu ~ normal(10, 2);
    sigma ~ gamma(2,2);
  }
"

data = list(y=c(7.1,8.9,8.4,8.6))

sim <- stan(model_code = model, data = data, iter = 5000*2, seed = 84735)
plot(sim)
```

# Chapter 7

## 7.6

a.  

```{r}
current <- 4.6

set.seed(84735)

proposal <- rnorm(1, current, 2)
proposal
```

b.  

```{r}
current <- 2.1

set.seed(84735)

proposal <- rnorm(1, current, 7)
proposal
```

c.  

```{r}
current <- 8.9

set.seed(84735)

proposal <- runif(1, min = current - 2, max = current + 2)
proposal
```

d.  

```{r}
current <- 1.2

set.seed(84735)

proposal <- runif(1, min = current - 0.5, max = current + 0.5)
proposal
```

e.  

```{r}
current <- 7.7

set.seed(84735)

proposal <- runif(1, min = current - 3, max = current + 3)
proposal
```

## 7.7

a.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {lambda^(-2)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

b.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {exp(lambda)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

c.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {exp(-10*lambda)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

d.  

```{r}
current <- 2
proposal <- 2.1
posterior <- function(lambda) {exp((-lambda)^4)} 

proposal_plaus <- posterior(proposal) * dexp(current, rate = proposal)

current_plaus  <- posterior(current) * dexp(proposal, rate = current)

alpha <- min(1, proposal_plaus / current_plaus) 
alpha
```

e.  

b and d are 1. The min function sets that any ratio (proposal plausibility to current plausibility) above 1 is 1; 1 is the ceiling. This is because 1 signifies the probability that we are going to accept the proposed location, which cannot be higher than 100%. In other words, 1 means that there is a 100% probability of accepting the proposed location.

## 7.8

a.  

```{r}
current <- 1.8
proposal <- 1.6
posterior <- function(lambda) {lambda^(-1)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

b.  

```{r}
current <- 1.8
proposal <- 1.6
posterior <- function(lambda) {exp(3*lambda)}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

c.  

```{r}
current <- 1.8
proposal <- 1.6
posterior <- function(lambda) {exp(((-1.9)*lambda))}

proposal_plaus <- posterior(proposal) 

current_plaus  <- posterior(current) 

alpha <- min(1, proposal_plaus / current_plaus)
alpha
```

d.  

```{r}
current <- 1.8
proposal <- 1.6
posterior <- function(lambda) {exp((-lambda)^4)} 

proposal_plaus <- posterior(proposal) * dexp(current, rate = proposal)

current_plaus  <- posterior(current) * dexp(proposal, rate = current)

alpha <- min(1, proposal_plaus / current_plaus) 
alpha
```

e.  

a and c are 100% acceptance probability. We would certainly want to accept the proposed location in these scenarios because the proposed plausibility is better than the current plausibility. This means the proposed value makes a better posterior than the current one.

## 7.10

```{r}
one_mh_iteration <- function(w, current){
 # STEP 1: Propose the next chain location
 proposal <- runif(1, min = current - w, max = current + w)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour <- function(N, w){
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(0, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_iteration(w = w, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), mu))
}
```

a.

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 50, w = 50)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

b.  

```{r}
set.seed(84735)
mh_simulation_2 <- mh_tour(N = 50, w = 0.01)
```

```{r}
ggplot(mh_simulation_2, aes(x = iteration, y = mu)) + 
  geom_line()
```

c.  

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 1000, w = 50)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

d.  

```{r}
set.seed(84735)
mh_simulation_1 <- mh_tour(N = 1000, w = 0.01)
```

```{r}
ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + 
  geom_line()
```

e.  

"w" is the the Uniform half-width. The half-width defines the range of potential proposals. A small w sets a small range for the tour’s current location. With b, for example, the chain can only move in the range of 0.01 units each time. Due to this, proposals will be very close to the current location and will have a high acceptance rate. Proposals will therefore tend to be very close to the current location, and thus have similar posterior plausibility and a high probability of being accepted. Because of this, a lower w will result in a more detailed plot. 

On the other hand, a larger range will result in a chunkier plot with less detail across iterations. This can be seen in comparing a and b. a, with a w of 50, charts a chunky line. This is due to its larger range around the tour's current location. Proposals can be much higher and lower than the current location compared to b. These proposals can be far outside what is plausible, causing more of them than b to be rejected. 

f.  

Having a low w is very important in scenarios with more iterations. As seen in c and d, plot d has much more detail and gives us more detailed information. For example, mu is around 3.15 in d, where it is around 4 in c. d has more mu's across the iterations, which leads me to trust its average more. 

## 7.12

a.  

```{r}
one_mh_normal_iteration <- function(s, current){
 # STEP 1: Propose the next chain location
 proposal <- rnorm(1, mean = current, sd = s)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75)
 current_plaus  <- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75)
 alpha <- min(1, proposal_plaus / current_plaus)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 # Return the results
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
mh_tour_normal <- function(N, s){
  
  # 1. Start the chain at location 3
  current <- 3

  # 2. Initialize the simulation
  mu <- rep(NA, N)

  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_mh_normal_iteration(s = s, current = current)
    
    # Record next location
    mu[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = 1:N, mu))
}
```

a. 

```{r}
set.seed(84735)
mh_tour_normal(20, 0.01) |>
  gf_line(mu~iteration)
```

b.  

```{r}
set.seed(84735)
mh_tour_normal(20, 10) |>
  gf_line(mu~iteration)
```

c.  

```{r}
set.seed(84735)
mh_tour_normal(1000, 0.01) |>
  gf_line(mu~iteration)
```

d.  

```{r}
set.seed(84735)
mh_tour_normal(1000, 10) |>
  gf_line(mu~iteration)
```

e.  

The standard deviation dictates how many deviations from the current value that the next chain's value will be predicted from. When changing the standard deviation to a larger number, the range that the next predicted value will be in is broadened. With a larger standard deviation, more proposals also get rejected. This is why we see a larger range of values and places where the line is flat in b's plot compared to a's.

On the other hand, a smaller deviation like a's will result in a finer range for mu and more proposals are accepted (since they're so close to the current value). This is what dictates the smaller range and more detail in a's plot. 

f.  

```{r}
set.seed(84735)
mh_tour_normal(1000, 1) |>
  gf_line(mu~iteration)
```

1 would be a reasonable standard deviation, as seen in the plot above.

## 7.14

a.  

I would choose an exponential proposal model because lambda is greater than 0.

b.  

```{r}
one_iteration_exp <- function(a, b, current){
 # Propose the next chain location
 proposal <- rgamma(1, a, b)
  
 # Decide whether or not to go there
 proposal_plaus <- dgamma(proposal, 1, 0.1) * dexp(current, rate = proposal)
 proposal_q     <- dgamma(proposal, a, b)
 current_plaus  <- dgamma(current, 1, 0.1) * dexp(current, rate = proposal)
 current_q      <- dgamma(current, a, b)
 alpha <- alpha <- min(1, proposal_plaus / current_plaus) 
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 return(data.frame(proposal, alpha, next_stop))}
```

```{r}
exp_tour <- function(N, a, b){
  # 1. Start the chain at location 1
  current <- 4

  # 2. Initialize the simulation
  pi <- rep(NA, N)
  
  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- one_iteration_exp(a = a, b = b, current = current)
    
    # Record next location
    pi[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), pi))
}
```

```{r}
set.seed(84735)
exp_sim <- exp_tour(N = 1000, a = 1, b = .1)

ggplot(exp_sim, aes(x = iteration, y = pi)) + 
  geom_line() 
```

c.  

The posterior is about 1. 

d.  

```{r}
# Plot the results
ggplot(exp_sim, aes(x = pi)) + 
  geom_histogram(aes(y = ..density..), color = "white") + 
  stat_function(fun = dgamma, args = list(1, 0.1), color = "blue")
```
My Markov chain's approximation is pretty good. The histogram of my model follows the curve of my gamma distribution function. 
## 7.15

a. 

Question: Will I eat a pound of celery per day (counting two days)?

b.

I will eat all of a 1 pound bag of celery on one day. The second day I will eat 0 pounds of celery, since I will probably be sick of it by then. For some context, I bought celery that I need to eat soon before it expires.

π|(Y=1) ∼ Beta(3,4) 

c. 

I ate the 1 lb of celery on day 1. On day 2, I ate none. 

```{r}
data = c(1,0)
data
```

Y = 1 
n = 2

d.

```{r}
celery_one_iteration <- function(a, b, current){
 # STEP 1: Propose the next chain location
 proposal <- rbeta(1, a, b)
  
 # STEP 2: Decide whether or not to go there
 proposal_plaus <- dbeta(proposal, 2, 3) * dbinom(1, 2, proposal)
 proposal_q     <- dbeta(proposal, a, b)
 current_plaus  <- dbeta(current, 2, 3) * dbinom(1, 2, current)
 current_q      <- dbeta(current, a, b)
 alpha <- min(1, proposal_plaus / current_plaus * current_q / proposal_q)
 next_stop <- sample(c(proposal, current), 
                     size = 1, prob = c(alpha, 1-alpha))
  
 return(data.frame(proposal, alpha, next_stop))
}
```

```{r}
celery_tour <- function(N, a, b){
  # 1. Start the chain at location 0.5
  current <- 0.5

  # 2. Initialize the simulation
  pi <- rep(0, N)
  
  # 3. Simulate N Markov chain stops
  for(i in 1:N){    
    # Simulate one iteration
    sim <- celery_one_iteration(a = a, b = b, current = current)
    
    # Record next location
    pi[i] <- sim$next_stop
    
    # Reset the current location
    current <- sim$next_stop
  }
  
  # 4. Return the chain locations
  return(data.frame(iteration = c(1:N), pi))
}
```

I used the Beta-Binomial model. I chose it because it only has one parameter and includes the underlying proportion of success. This matches well with my data. 

Also, I use the independence sampling algorithm in the functions above. This is so that the proposal is not dependent on the current tour stop. 

e.

```{r}
set.seed(84735)
celery_sim <- celery_tour(N = 2000, a = 1, b = 1)

# Plot the results
ggplot(celery_sim, aes(x = iteration, y = pi)) + 
  geom_line()
```

f. 

```{r}
ggplot(celery_sim, aes(x = pi)) + 
  geom_histogram(aes(y = ..density..), bins = 15, color = "white") + 
  stat_function(fun = dbeta, args = list(3, 4), color = "blue")
```

The histogram illustrates the relative posteriors of celery eaten per day. Each proposal was equally likely to be anywhere between 0 and 1. As seen in the plot above, the proposals are centered around 0.4 lbs of celery eaten per day. 
