---
title: "ch8.rmd"
author: "Brea Koenes"
date: "10/24/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Load packages
library(bayesrules)
library(tidyverse)
library(ggformula)
library(dplyr)
library(bayesplot)
library(tidybayes)
library(posterior)
library(janitor)
library(rstan)
library(broom.mixed)
```

# Chapter 8

## 8.14

a.  

Beta-Binomial is the correct choice in model because the prior is binomial. Specifically, the subject either believes in climate change or does not.

b.  

My prior would be Beta(1,4). The mean of the prior is 0.20; 20% will not believe in climate change. My standard deviation is 4.

c.

My prior understanding is that, on average, less people will not believe in climate change. In other words, I think that more people believe that climate change is real than the authors think.

d.  

```{r}
summary(pulse_of_the_nation)
```

150/1000= 15% don't believe it's real at all

e.  

```{r}
summarize_beta_binomial(alpha = 1, beta = 2, y = 150, n = 1000)
```

```{r}
qbeta(c(0.25, 0.975), 151, 852)
```

There is a 95% probability that the true proportion of people that don't believe in climate change lies within 14-17%. 

## 8.15

a.  

Given the credible interval above, it is likely that the true proportion lies within 14-17%. The null hypothesis states that the proportion is equal to or lower than 10%. The alternative hypothesis is that the proportion is greater than 10%. With this information, I would personally raise the null hypothesis to be equal to or lower than 0.14, and the laternative hypothesis greater than 0.14.

b.  

```{r}
prior_prob <- pbeta(0.10, 1, 2)
prior_odds <- prior_prob / (1 - prior_prob)

post_prob <- 1-prior_prob 
post_prob
```

```{r}
post_odds <- post_prob / (1 - post_prob)
post_odds
```

c.  

```{r}
BF <- post_odds / prior_odds
BF
```

The posterior odds of our hypothesis are roughly 18 times higher than the prior odds

d.  

Our alternative hypothesis that pi is greater than 10% is quite likely to occur. In the context of our scenario, it is very likely the true proportion of people that don't believe in climate change is over 10%. 

## 8.16

a.  

```{r}
climate_model <- "
  data {
    int<lower = 0, upper = 1000> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(150, pi);
    pi ~ beta(1,2);
  }
"

# STEP 2: SIMULATE the posterior
sim <- stan(model_code = climate_model, 
             data = list(Y = 150), 
             chains = 4, 
             iter = 5000*2, 
             seed = 84735)
```
b.  

```{r}
require(bayesplot)

# Parallel trace plots & density plots
mcmc_trace(sim, pars = "pi", size = .5) + 
  xlab("iteration")
```

```{r}
mcmc_dens_overlay(sim, pars = "pi")
```

```{r}
# Autocorrelation plot
mcmc_acf(sim, pars = "pi")
```

c.  

```{r}
neff_ratio(sim, pars = "pi")
```

```{r}
stan_rhat(sim, par = "pi")
```

```{r}
summary(sim)
```

R-hat is a convergence diagnostic; it tells us whether or not an MCMC algorithm has converged flag situations where the algorithm has failed converge. Effective sample size tells us the effectiveness of a sample chain; it estimates the sample size required to achieve the same level of precision if the pulse of the sample used was a simple random sample. 

## 8.17

a.  

```{r}
tidy(sim, conf.int = TRUE, conf.level = 0.95)
```

b.  

```{r}
chains_df <- as.data.frame(sim, pars = "lp__", include = FALSE)
```

```{r}
chains_df |>
  mutate(exceeds = pi > 0.1) |> 
  tabyl(exceeds)
```

c.  

The approximations in parts a and b are fairly close to the actual posterior values calculated in Exercises 8.14 and 8.15.

## 8.18

a.  

```{r}
set.seed(1)

# Predict a value of Y' for each pi value in the chain
chains_df <- chains_df |>
  mutate(y_predict = rbinom(length(pi), size = 100, prob = pi))
```

b.  

```{r}
summary(chains_df)
```

c.  

```{r}
chains_df |>
  summarize(mean = mean(y_predict),
            lower_20 = quantile(y_predict, 0.2),
            upper_20 = quantile(y_predict, 0.8))
```

## 8.21

a.  

```{r}
glimpse(loons)
```


```{r}
gf_point(count_per_100~hours, data=loons)
```

Looking at the distribution of data, I would choose a Gamma-Poisson model. The Poisson model won't predict negative values and the shape of its distribution changes. 

b.  

My prior is (2, 1). My reasoning is that the typical rate of loon sightings is 2 per 100 hours, giving us the first part of our prior. For the second, I chose 1/100 due to there being a standard deviation of 1 per 100-hours.

c.  

```{r}
summary(loons)
```

```{r}
loons
```

The mean count per 100 is 1.5. There are observations of count_per_100, including 0's. 

d.  

```{r}
summarize_gamma_poisson(shape = 2, rate = 1, sum_y = 1.5, n = 3)
```

```{r}
qbeta(c(0.25, 0.975), 3.5, 4)
```


There is a 95% probability that the true rate of loons sightings across a 100-hour observation period lies within .34-.80. 

## 8.22

a.  

$$
H_0 = \lambda >= 0.01 \\
H_a = \lambda >= 0.01 
$$

b. 

I would say that the alternative hypothesis is too low. The credible interval indicates that the actual rate is probable to be larger than 10, not smaller as the alternative hypothesis states. I would say that the alternative hypothesis is not very likely. 

c.  

```{r}
post_prob <- pbeta(0.01, 2, 1)
post_prob
```

The posterior probability is 0.0001. This means that our alternative hypothesis is not likely at all to be true. 

d.  

My conclusion about lambda is that the null hypothesis is much more likely than the alternative hypothesis. This means that the true proportion of loon sightings in a 100-hour observation period is likely to be greater than 1%. 

## 8.23

a.  

```{r}
# Define model
loon_model <- "
  data {
    int<lower = 0> n;
    int<lower = 0> Y[n];
  }
  parameters {
    real<lower = 0> lambda;
  }
  model {
    Y ~ poisson(lambda); 
    lambda ~ gamma(2, 1);
  }
"

# Simulate the posterior
loon_sim <- stan(model_code = loon_model, data = list(Y = loons$count, n = nrow(loons)), 
               chains = 4, iter = (5000*2), seed = 84735)
```

b.  

```{r}
mcmc_trace(loon_sim, pars = "lambda")
```

```{r}
mcmc_dens_overlay(loon_sim, pars = "lambda")
```

```{r}
mcmc_acf(loon_sim, pars = "lambda")
```

```{r}
neff_ratio(loon_sim, pars = "lambda")
```

c.  

```{r}
tidy(loon_sim, conf.int = TRUE, conf.level = 0.95)
```

```{r}
mcmc_areas(loon_sim, pars = "lambda", prob = 0.95)
```

d.  

```{r}
loon_chains_df <- as.data.frame(loon_sim, pars = "lp__", include = FALSE)

loon_chains_df |> 
  mutate(exceeds = lambda < 1) |> 
  tabyl(exceeds)
```

e.  

The approximations in c and d are fairly similar to mine in 8.21 and 8.22. They're all around 0.80. Also, I couldn't figure out how to input the loons dataset as the data in my Markov model, so that's probably causing some variation as well.

## 8.24

a.  

```{r}
set.seed(1)

loon_chains_df <- loon_chains_df |> 
  mutate(y_predict = rgamma(shape = 2, length(lambda)))

loon_chains_df |> 
  head(3)
```

```{r}
ggplot(loon_chains_df, aes(x = y_predict)) + 
  stat_count()
```

b.  

```{r}
loon_chains_df |> 
  summarize(mean = mean(y_predict),
            lower_80 = quantile(y_predict, 0.1),
            upper_80 = quantile(y_predict, 0.9))
```

c.  

```{r}
loon_chains_df <- as.data.frame(loon_sim, pars = "lp__", include = FALSE)

loon_chains_df |> 
  mutate(zero = lambda == 0) |> 
  tabyl(zero)
```

