---
html_document:
  code_download: yes
author: "Brea Koenes"
date: "10/19/2022"
output:
  html_document:
    df_print: paged
title: "Stat341 Portfolio"
---

```{r setup, include=FALSE}
library(bayesrules)
library(tidyverse)
library(ggformula)
library(dplyr)
```

# Chapter 4

## 4.5

#### Kimya Beta(1, 2)

```{r kimya a}
set.seed(84735)
kimya_sim <- data.frame(pi = rbeta(10000, 1, 2)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = 3/7))
```

```{r kimya b}
kimya_hist <- gf_histogram(~y, data = kimya_sim, geom = "bar")
```

```{r kimya c}
set.seed(84735)
kimya_pi_sim <- data.frame(pi = rbeta(10000, 1, 2)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = pi))
```

#### Fernando Beta(0.5, 1)

```{r fernando a}
set.seed(84735)
fernando_sim <- data.frame(pi = rbeta(10000, .5, 1)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = 3/7))
```

```{r fernando b}
fernando_hist <- gf_histogram(~y, data = fernando_sim, geom = "bar")
```

```{r fernando c}
set.seed(84735)
fernando_pi_sim <- data.frame(pi = rbeta(10000, .5, 1)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = pi))
```

#### Ciara Beta(3, 10)

```{r ciara a}
set.seed(84735)
ciara_sim <- data.frame(pi = rbeta(10000, 3, 10)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = 3/7))
```

```{r ciara b}
ciara_hist <- gf_histogram(~y, data = ciara_sim, geom = "bar")
```

```{r ciara c}
set.seed(84735)
ciara_pi_sim <- data.frame(pi = rbeta(10000, 3, 10)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = pi))
```

#### Taylor Beta(2, 0.1)

```{r taylor a}
set.seed(84735)
taylor_sim <- data.frame(pi = rbeta(10000, 2, .1)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = 3/7))
```

```{r taylor b}
taylor_hist <- gf_histogram(~y, data = taylor_sim, geom = "bar")
```

```{r taylor c}
set.seed(84735)
taylor_pi_sim <- data.frame(pi = rbeta(10000, 2, .1)) %>% 
  mutate(y = rbinom(10000, size = 7, prob = pi))
```

## 4.14

a.  In the Beta-Binomial setting, show that we can write the posterior mode of pi as the weighted average of the prior mode and observed sample success rate:

\$\$ \begin{align*}

Mode(\pi∣Y=y) &= (\alpha+y−1)/(\alpha+\beta+n−2) \\
&= \frac{\alpha−1}{\alpha+\beta+n−2} + y/(\alpha+\beta+n−2) \\
&= (\alpha−1)/(\alpha+\beta+n−2) ⋅ (\alpha+\beta−2)/(\alpha+\beta−2) + y/(\alpha+\beta+n−2) ⋅ n/n \\
&= (\alpha+\beta−2)/(\alpha+\beta+n−2) ⋅ (\alpha−1)/(\alpha+\beta−2) + n/(\alpha+\beta+n−2) ⋅ y/n \\
&= (\alpha+\beta−2) / (\alpha+\beta+n−2) ⋅ Mode(\pi) + n/(\alpha+\beta+n−2) ⋅ y/n \\

\end{align*} \$\$

b.  To what value does the posterior mode converge as our sample size n increases? Support your answer with evidence.

{w1, w2,...} consists of a countable (possibly infinite) set of values

\$\$

\displaystyle \\lim*{n* \to \\infty} Pr(w_t\|x_1,...x_n)=1, \\ \displaystyle \\lim{n \to \\infty} Pr(w_i\|x_1,...x_n)=0, \\ i!=t

\$\$

Using the strong law of large numbers and given that there are appropriate regularity conditions, it can be shown that the posterior converges to 1 as n increases.

# Chapter 5

## 5.10

a.  

```{r}
data <- c(-0.7,1.2,4.5,-4)
plot_normal_likelihood(y = data, sigma = 4)
```

b.  

```{r}
bayesrules::plot_beta(alpha = 2, beta = 7, mean = TRUE, mode = TRUE)
```

```{r}
plot_normal_normal(mean = 7, sd = 2, sigma = 2.6, y_bar = 7.2, n = 4)
```

c.  

```{r}
bayesrules::summarize_beta(alpha = 2, beta = 7)
```

```{r}
summarize_normal_normal(mean = 7, sd = 2, sigma = 2.6, y_bar = 7.2, n = 4)
```

d.  

My understanding of the mean evolved from the prior by the posterior's mean being below .25 while the mean of the prior was .25.

e.  

```{r}
pnorm(0, mean=7,sd=2, log.p=FALSE)
```

f.  

```{r}
pnorm(8, mean=7,sd=2, log.p=FALSE)
```

## 5.11

a.  

```{r}
summarize_normal_normal(mean = 80, sd = 4, sigma = 3, y_bar = 86, n = 32)
```

b.  

```{r}
summarize_normal_normal(mean = 80, sd = 4, sigma = 3, y_bar = 82, n = 32)
```

c.  

```{r}
summarize_normal_normal(mean = 80, sd = 4, sigma = 3, y_bar = 84, n = 32)
```

## 5.13

a.  

```{r}
plot_normal(mean = 30, sd = 5)
```

```{r}
bayesrules::plot_beta(alpha = 5, beta = 30, mean = TRUE, mode = TRUE)
```

b.  

```{r}
ggplot(weather_perth, aes(x = temp3pm)) + 
  geom_density()
```

It is reasonable to assume a Normal model for the temperature data due to the patterns observed in the density plot.

c.  

```{r}
summarize_normal_normal(mean = 30, sd = 5, sigma = 3, y_bar = 40, n = 1000)
```

d.  

```{r}
plot_normal_normal(mean = 30, sd = 5, sigma = 3,
                   y_bar = 40, n = 1000)
```

I expected the posterior mean to be higher than the prior, but I did not expect it to be that much higher. Also, did anticipate the sd and variance being much lower for the posterior compared to the prior.
